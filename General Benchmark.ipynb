{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model llama3.2:1b Accuracy on ARC-Challenge (100 questions): 42.00% ===\n"
     ]
    }
   ],
   "source": [
    "# ======================== Bechmarking of model ============================\n",
    "\n",
    "# Run the evaluation on the ARC-Challenge dataset\n",
    "from datasets import load_dataset\n",
    "import ollama\n",
    "\n",
    "def format_prompt(q, choices):\n",
    "    letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
    "    options = \"\\n\".join([f\"{l}. {c}\" for l, c in zip(letters, choices)])\n",
    "    return f\"Question: {q}\\nChoices:\\n{options}\\nAnswer (just the letter):\"\n",
    "\n",
    "def extract_letter_from_response(resp):\n",
    "    for c in resp.strip().upper():\n",
    "        if c in \"ABCDEF\":\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def run_arc_evaluation(model=\"llama3.2:1b\", subset=\"ARC-Challenge\", n=100): # llama3.2:1b or llama3.2:3b or llama3-1b-spamgen\n",
    "    dataset = load_dataset(\"ai2_arc\", subset, split=\"train\")\n",
    "    correct = 0\n",
    "    total = min(n, len(dataset))\n",
    "\n",
    "    for i in range(total):\n",
    "        row = dataset[i]\n",
    "        q = row[\"question\"]\n",
    "        choices = row[\"choices\"][\"text\"]\n",
    "        correct_answer = row[\"answerKey\"]\n",
    "\n",
    "        prompt = format_prompt(q, choices)\n",
    "\n",
    "        result = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.7, \"max_tokens\": 10}\n",
    "        )\n",
    "        answer = extract_letter_from_response(result[\"message\"][\"content\"])\n",
    "\n",
    "        is_correct = answer == correct_answer\n",
    "        if is_correct: correct += 1\n",
    "\n",
    "        #print(f\"\\nQ{i+1}: {q}\")\n",
    "        #full_output = result[\"message\"][\"content\"]\n",
    "        #print(f\"Full model output:\\n{full_output}\")  # <<< ici tu vois exactement ce que le modèle répond\n",
    "        #print(f\"Extracted Answer: {answer} | Expected: {correct_answer} | {'✅' if answer == correct_answer else '❌'}\")\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"\\n=== Model {model} Accuracy on {subset} ({total} questions): {acc:.2f}% ===\")\n",
    "\n",
    "run_arc_evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model llama3-1b-spamgen Accuracy on ARC-Challenge (100 questions): 46.00% ===\n"
     ]
    }
   ],
   "source": [
    "run_arc_evaluation(model=\"llama3-1b-spamgen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model llama3.2:3b Accuracy on ARC-Challenge (100 questions): 70.00% ===\n"
     ]
    }
   ],
   "source": [
    "run_arc_evaluation(model=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model llama3.2:1b Accuracy on ARC-Challenge (100 questions): 41.00% ===\n"
     ]
    }
   ],
   "source": [
    "run_arc_evaluation(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33766\\Documents\\Travail\\TU Delft\\Q3\\Machine Learning Bayesian\\Assignment2ML\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==== Running evaluation at temperature 0.0 ====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Temp 0.0: 100%|██████████| 100/100 [13:34<00:00,  8.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Accuracy per Model ===\n",
      "llama3-1b-spamgen: 47.00%\n",
      "llama3.2:3b: 72.00%\n",
      "llama3.2:1b: 41.00%\n",
      "\n",
      "=== Agreement Between Models ===\n",
      "llama3-1b-spamgen ↔ llama3.2:3b: 49.00%\n",
      "llama3-1b-spamgen ↔ llama3.2:1b: 38.00%\n",
      "llama3.2:3b ↔ llama3.2:1b: 48.00%\n",
      "\n",
      "=== Shared Errors / Correct Statistics ===\n",
      "All models wrong: 16/100\n",
      "All models correct: 24/100\n",
      "Partial disagreement: 60/100\n",
      "Saved detailed results to arc_eval_temp0.0.csv\n",
      "\n",
      "\n",
      "==== Running evaluation at temperature 0.5 ====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Temp 0.5: 100%|██████████| 100/100 [13:26<00:00,  8.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Accuracy per Model ===\n",
      "llama3-1b-spamgen: 38.00%\n",
      "llama3.2:3b: 74.00%\n",
      "llama3.2:1b: 43.00%\n",
      "\n",
      "=== Agreement Between Models ===\n",
      "llama3-1b-spamgen ↔ llama3.2:3b: 43.00%\n",
      "llama3-1b-spamgen ↔ llama3.2:1b: 34.00%\n",
      "llama3.2:3b ↔ llama3.2:1b: 49.00%\n",
      "\n",
      "=== Shared Errors / Correct Statistics ===\n",
      "All models wrong: 16/100\n",
      "All models correct: 21/100\n",
      "Partial disagreement: 63/100\n",
      "Saved detailed results to arc_eval_temp0.5.csv\n",
      "\n",
      "\n",
      "==== Running evaluation at temperature 0.9 ====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Temp 0.9: 100%|██████████| 100/100 [13:10<00:00,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Accuracy per Model ===\n",
      "llama3-1b-spamgen: 29.00%\n",
      "llama3.2:3b: 73.00%\n",
      "llama3.2:1b: 38.00%\n",
      "\n",
      "=== Agreement Between Models ===\n",
      "llama3-1b-spamgen ↔ llama3.2:3b: 36.00%\n",
      "llama3-1b-spamgen ↔ llama3.2:1b: 32.00%\n",
      "llama3.2:3b ↔ llama3.2:1b: 39.00%\n",
      "\n",
      "=== Shared Errors / Correct Statistics ===\n",
      "All models wrong: 14/100\n",
      "All models correct: 11/100\n",
      "Partial disagreement: 75/100\n",
      "Saved detailed results to arc_eval_temp0.9.csv\n",
      "\n",
      "\n",
      "==== Summary Table Across Temperatures ====\n",
      " temperature  acc_llama3-1b-spamgen  acc_llama3.2:3b  acc_llama3.2:1b  agree_llama3-1b-spamgen_llama3.2:3b  agree_llama3-1b-spamgen_llama3.2:1b  agree_llama3.2:3b_llama3.2:1b  all_correct  all_wrong  partial_disagreement\n",
      "         0.0                   47.0             72.0             41.0                                 49.0                                 38.0                           48.0           24         16                    60\n",
      "         0.5                   38.0             74.0             43.0                                 43.0                                 34.0                           49.0           21         16                    63\n",
      "         0.9                   29.0             73.0             38.0                                 36.0                                 32.0                           39.0           11         14                    75\n",
      "Saved summary table to arc_summary_all_temperatures.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>acc_llama3-1b-spamgen</th>\n",
       "      <th>acc_llama3.2:3b</th>\n",
       "      <th>acc_llama3.2:1b</th>\n",
       "      <th>agree_llama3-1b-spamgen_llama3.2:3b</th>\n",
       "      <th>agree_llama3-1b-spamgen_llama3.2:1b</th>\n",
       "      <th>agree_llama3.2:3b_llama3.2:1b</th>\n",
       "      <th>all_correct</th>\n",
       "      <th>all_wrong</th>\n",
       "      <th>partial_disagreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temperature  acc_llama3-1b-spamgen  acc_llama3.2:3b  acc_llama3.2:1b  \\\n",
       "0          0.0                   47.0             72.0             41.0   \n",
       "1          0.5                   38.0             74.0             43.0   \n",
       "2          0.9                   29.0             73.0             38.0   \n",
       "\n",
       "   agree_llama3-1b-spamgen_llama3.2:3b  agree_llama3-1b-spamgen_llama3.2:1b  \\\n",
       "0                                 49.0                                 38.0   \n",
       "1                                 43.0                                 34.0   \n",
       "2                                 36.0                                 32.0   \n",
       "\n",
       "   agree_llama3.2:3b_llama3.2:1b  all_correct  all_wrong  partial_disagreement  \n",
       "0                           48.0           24         16                    60  \n",
       "1                           49.0           21         16                    63  \n",
       "2                           39.0           11         14                    75  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================== Advanced Multi-Model Multi-Temperature Benchmark (with progress bar) ============================\n",
    "from datasets import load_dataset\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODELS = [\"llama3-1b-spamgen\", \"llama3.2:3b\", \"llama3.2:1b\"]\n",
    "TEMPERATURES = [0.0, 0.5, 0.9]\n",
    "SUBSET = \"ARC-Challenge\"\n",
    "N_QUESTIONS = 100\n",
    "\n",
    "def format_prompt(q, choices):\n",
    "    letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
    "    options = \"\\n\".join([f\"{l}. {c}\" for l, c in zip(letters, choices)])\n",
    "    return f\"Question: {q}\\nChoices:\\n{options}\\nAnswer (just the letter):\"\n",
    "\n",
    "def extract_letter_from_response(resp):\n",
    "    for c in resp.strip().upper():\n",
    "        if c in \"ABCDEF\":\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def run_full_benchmark(models=MODELS, temperatures=TEMPERATURES, subset=SUBSET, n=N_QUESTIONS):\n",
    "    dataset = load_dataset(\"ai2_arc\", subset, split=\"train\")\n",
    "    total = min(n, len(dataset))\n",
    "    summary_stats = []\n",
    "\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n\\n==== Running evaluation at temperature {temp:.1f} ====\\n\")\n",
    "        results = defaultdict(list)\n",
    "\n",
    "        for i in tqdm(range(total), desc=f\"Temp {temp:.1f}\"):\n",
    "            q = dataset[i][\"question\"]\n",
    "            choices = dataset[i][\"choices\"][\"text\"]\n",
    "            correct_answer = dataset[i][\"answerKey\"]\n",
    "            prompt = format_prompt(q, choices)\n",
    "\n",
    "            row = {\"question_id\": i, \"question\": q, \"correct\": correct_answer}\n",
    "            for model in models:\n",
    "                response = ollama.chat(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    options={\"temperature\": temp, \"max_tokens\": 10}\n",
    "                )\n",
    "                answer = extract_letter_from_response(response[\"message\"][\"content\"])\n",
    "                row[model] = answer\n",
    "\n",
    "            results[\"all\"].append(row)\n",
    "\n",
    "        df = pd.DataFrame(results[\"all\"])\n",
    "        for model in models:\n",
    "            df[f\"correct_{model}\"] = df[model] == df[\"correct\"]\n",
    "\n",
    "        print(\"=== Accuracy per Model ===\")\n",
    "        acc_dict = {\"temperature\": temp}\n",
    "        for model in models:\n",
    "            acc = df[f\"correct_{model}\"].mean()\n",
    "            acc_dict[f\"acc_{model}\"] = round(acc * 100, 2)\n",
    "            print(f\"{model}: {acc:.2%}\")\n",
    "\n",
    "        print(\"\\n=== Agreement Between Models ===\")\n",
    "        for i in range(len(models)):\n",
    "            for j in range(i+1, len(models)):\n",
    "                m1, m2 = models[i], models[j]\n",
    "                agreement = (df[m1] == df[m2]).mean()\n",
    "                print(f\"{m1} ↔ {m2}: {agreement:.2%}\")\n",
    "                acc_dict[f\"agree_{m1}_{m2}\"] = round(agreement * 100, 2)\n",
    "\n",
    "        print(\"\\n=== Shared Errors / Correct Statistics ===\")\n",
    "        all_wrong = (~df[f\"correct_{models[0]}\"] & ~df[f\"correct_{models[1]}\"] & ~df[f\"correct_{models[2]}\"]).sum()\n",
    "        all_correct = (df[f\"correct_{models[0]}\"] & df[f\"correct_{models[1]}\"] & df[f\"correct_{models[2]}\"]).sum()\n",
    "        partial_overlap = total - all_wrong - all_correct\n",
    "        print(f\"All models wrong: {all_wrong}/{total}\")\n",
    "        print(f\"All models correct: {all_correct}/{total}\")\n",
    "        print(f\"Partial disagreement: {partial_overlap}/{total}\")\n",
    "        acc_dict[\"all_correct\"] = all_correct\n",
    "        acc_dict[\"all_wrong\"] = all_wrong\n",
    "        acc_dict[\"partial_disagreement\"] = partial_overlap\n",
    "\n",
    "        filename = f\"arc_eval_temp{temp:.1f}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved detailed results to {filename}\")\n",
    "\n",
    "        summary_stats.append(acc_dict)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    print(\"\\n\\n==== Summary Table Across Temperatures ====\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    summary_df.to_csv(\"arc_summary_all_temperatures.csv\", index=False)\n",
    "    print(\"Saved summary table to arc_summary_all_temperatures.csv\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# Run all benchmarks\n",
    "run_full_benchmark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1/3\n",
      "llama3-1b-spamgen Accuracy (Run 1): 50.00%\n",
      "llama3.2:1b Accuracy (Run 1): 15.00%\n",
      "\n",
      "Run 2/3\n",
      "llama3-1b-spamgen Accuracy (Run 2): 50.00%\n",
      "llama3.2:1b Accuracy (Run 2): 35.00%\n",
      "\n",
      "Run 3/3\n",
      "llama3-1b-spamgen Accuracy (Run 3): 45.00%\n",
      "llama3.2:1b Accuracy (Run 3): 35.00%\n",
      "\n",
      "Average Accuracies Across Runs:\n",
      "llama3-1b-spamgen: 48.33%\n",
      "llama3.2:1b: 28.33%\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import ollama\n",
    "import numpy as np\n",
    "\n",
    "MODELS = [\"llama3-1b-spamgen\", \"llama3.2:1b\"]\n",
    "TEMPERATURE = 0.0\n",
    "N_RUNS = 3\n",
    "QUESTIONS_PER_RUN = 20\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"train\")\n",
    "total_questions = len(dataset)\n",
    "\n",
    "accuracies = {model: [] for model in MODELS}\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    # Ensure different questions for each run\n",
    "    indices = np.random.choice(total_questions, QUESTIONS_PER_RUN, replace=False)\n",
    "    correct_counts = {model: 0 for model in MODELS}\n",
    "\n",
    "    print(f\"\\nRun {run + 1}/{N_RUNS}\")\n",
    "    for idx in indices:\n",
    "        question_data = dataset[int(idx)]\n",
    "        q = question_data[\"question\"]\n",
    "        choices = question_data[\"choices\"][\"text\"]\n",
    "        correct_answer = question_data[\"answerKey\"]\n",
    "        prompt = f\"Question: {q}\\nChoices:\\n\" + \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)]) + \"\\nAnswer (just the letter):\"\n",
    "\n",
    "        for model in MODELS:\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": TEMPERATURE, \"max_tokens\": 10}\n",
    "            )\n",
    "            predicted_answer = next((c for c in response[\"message\"][\"content\"].strip().upper() if c in \"ABCDEF\"), None)\n",
    "            correct_counts[model] += (predicted_answer == correct_answer)\n",
    "\n",
    "    # Calculate accuracy for this run\n",
    "    for model in MODELS:\n",
    "        accuracy = correct_counts[model] / QUESTIONS_PER_RUN\n",
    "        accuracies[model].append(accuracy)\n",
    "        print(f\"{model} Accuracy (Run {run + 1}): {accuracy:.2%}\")\n",
    "\n",
    "# Final accuracy averaged over runs\n",
    "print(\"\\nAverage Accuracies Across Runs:\")\n",
    "for model in MODELS:\n",
    "    avg_accuracy = np.mean(accuracies[model])\n",
    "    print(f\"{model}: {avg_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
